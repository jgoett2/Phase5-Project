{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Curriculum Rigor\n",
    "## Background\n",
    "In my experience with high school curriculum, I have found a wide variation in the rigor of course material.  This project seeks to develop a tool for evaluating the rigor of a curriculum, by measuring its alignment to the College Board's respective AP Course.  This project focuses on the College Board's AP Computer Science A course, which covers a first year Java and Object Orientied Design course.\n",
    "\n",
    "For this course, the College Board defines a set of \"Computational Thinking Practices\" (skills) and content that will be assessed on a year-end summative assessment to determine student's mastery of the course.   \n",
    "\n",
    "There are 5 main Computational Thinking Practices identified by the College Board, which it then breaks down into subskills:  \n",
    "\n",
    "<img src=\"Reports/Images/Skills-List.png\" width=600px> \n",
    "\n",
    "In addition, the College Board defines a set of \"Essential Knowledge\" (the content) to be assessed in the course, which it organizes under 5 \"Big Ideas.\"  For example, the content for a lesson on iteration is: \n",
    "\n",
    "<img src=\"Reports/Images/Content-Sample.png\" width=400px>\n",
    "\n",
    "Every question on the College Board's end-of-course summative exam is aligned to a particular computational thinking skill and essential knowledge.  As a note, some school networks have found the College Board's standards to be very complete, and \"backwards plan\" their middle school and pre-AP high school courses to prepare students for the AP level work. \n",
    "\n",
    "As a first step, this project will focus on the assessment questions used in a particular curriculum, and measure how well they align to the College Board's Computational Thinking Practice and Curriculum Framework.  (As a note, AP classes in most subjects have an analagous set of  thinking practices and framework standards, so one day, this work may be generalized to assess curriculums in other subject areas.)\n",
    "\n",
    "Two questions to assess are:  \n",
    "\n",
    "1. Can a TF-IDF vectorization of College Board question prompt with a Logistic Regressor successfully classify an assessment question by Computational Thinking Practice?\n",
    "2. If ChatGPT is supplied only with the College Board Framework for Computational Thinking, can it successfully identify the particular thinking practice being assessed by a question prompt?\n",
    "\n",
    "### Initial Conclusions:\n",
    "1. The TF-IDF and Logistic Regression together classify questions with a 74% accuracy rate when trained on just prompts.\n",
    "2. ChatGPT, supplied with the College Board Framework, classify with a 47% accuracy. \n",
    "3. Logistic Regression does not benefit from supplying the entire test question, as it selects erroneous words as signifiers.  Need more test data.\n",
    "\n",
    "### Next Steps:\n",
    "1. Determine whether the classifier can also identify the \"Essential Knowledge\" assessed by the question, not just the computational skill.\n",
    "2. Attempt to generalize the classifiers to classify non-assessment questions such as lecture material, lab questions, and homework problems.\n",
    "3. Create a visualization that shows the distribution of thinking skills and content assessed over the course of the curriculum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying Questions Using Logistic Regression\n",
    "As first step, this section will try to classify prompts as assessing one of these two AP Computational Thinking Practices (CTP):\n",
    "1. **CTP 2.A**: Apply the meaning of specific operators.  For example:  \n",
    "*Consider the following code segment.*  \n",
    "```\n",
    "int x = 7;  \n",
    "int y = 3;  \n",
    "if ((x < 10) && (y < 0))  \n",
    "  System.out.println(\"\"Value is: \"\" + x * y);  \n",
    "else  \n",
    "  System.out.println(\"\"Value is: \"\" + x / y) \n",
    "```\n",
    "*What is printed as a result of executing the code segment?*\n",
    "\n",
    "2. **CTP 2.B**: Determine the results or output based on statement execution order in a code segment without method calls (except for output).  For example:  \n",
    "\n",
    "*Consider the following code segment.* \n",
    "```\n",
    "int[] arr = {7, 2, 5, 3, 0, 10};  \n",
    "for (int k = 0; k < arr.length - 1; k++)  {  \n",
    "  if (arr[k] > arr[k + 1])  \n",
    "    System.out.print(k + \"\" \"\" + arr[k] + \"\" \"\");  \n",
    "} \n",
    "``` \n",
    "*What will be printed as a result of executing the code segment?*  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the Data\n",
    "In this first step, we will:\n",
    "1. Read in example prompts for each category.\n",
    "2. Preprocess the data: tokenize, lemmatize, and look at the token frequency distribution by question category.\n",
    "3. Build and test a Logistic Regression Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"Data/Synthetic/Synthetic2A.csv\")\n",
    "df2 = pd.read_csv(\"Data/Synthetic/Synthetic2B.csv\")\n",
    "df = pd.concat([df1, df2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jgoett/anaconda3/envs/learn-env/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/learn-env/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "basic_token_pattern = r\"(?u)\\b\\w\\w+\\b\"\n",
    "\n",
    "tokenizer = RegexpTokenizer(basic_token_pattern)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    temp = tokenizer.tokenize(text.lower())\n",
    "    return [lemmatizer.lemmatize(w) for w in temp]\n",
    "\n",
    "tfidf = TfidfVectorizer(strip_accents='ascii', tokenizer=lemmatize_text, stop_words=\"english\", max_features=10)\n",
    "\n",
    "result = tfidf.fit_transform(df[\"Question\"])\n",
    "\n",
    "X = pd.DataFrame(result.toarray(), columns=tfidf.get_feature_names_out())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [consider, the, following, code, segment, assu...\n",
       "1    [consider, the, following, code, segment, assu...\n",
       "2    [consider, the, following, code, segment, assu...\n",
       "3    [consider, the, following, code, segment, assu...\n",
       "4    [consider, the, following, code, segment, assu...\n",
       "5    [consider, the, following, code, segment, assu...\n",
       "6    [consider, the, following, code, segment, assu...\n",
       "7    [consider, the, following, code, segment, assu...\n",
       "8    [consider, the, following, code, segment, assu...\n",
       "9    [consider, the, following, code, segment, assu...\n",
       "0    [consider, the, following, code, segment, int,...\n",
       "1    [consider, the, following, code, segment, int,...\n",
       "2    [consider, the, following, code, segment, int,...\n",
       "3    [consider, the, following, code, segment, int,...\n",
       "4    [consider, the, following, code, segment, stri...\n",
       "5    [consider, the, following, code, segment, int,...\n",
       "6    [consider, the, following, code, segment, char...\n",
       "7    [consider, the, following, code, segment, int,...\n",
       "8    [consider, the, following, code, segment, int,...\n",
       "9    [consider, the, following, code, segment, int,...\n",
       "Name: Question_lem, dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Question_lem\"] = df[\"Question\"].apply(lemmatize_text)\n",
    "df[\"Question_lem\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Test a Logistic Regression Classifier on the Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, df[\"Classification\"])\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "lr.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         2.A       1.00      1.00      1.00         2\n",
      "         2.B       1.00      1.00      1.00         3\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = lr.predict(X_test)\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

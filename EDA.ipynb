{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Curriculum Rigor\n",
    "In my experience with high school curriculum, I have found a wide variation in the rigor of course material.  This project seeks to develop a tool for evaluating the alignment of a particular curriculum to the College Board's AP Computer Science A framework.  As a first step, this project poses two different questions:\n",
    "\n",
    "1. Can a TF-IDF vectorization of College Board questions with a Logistic Regressor successfully classify an assessment question by Computational Thinking Practice?\n",
    "2. If ChatGPT is supplied with the College Board Framework for Computational Thinking, can it successfully identify the particular thinking practice being assessmed by a question?\n",
    "\n",
    "### Initial Conclusions:\n",
    "1. The TF-IDF and Logistic Regression together classify questions with a 74% accuracy rate.\n",
    "2. ChatGPT, supplied with the College Board Framework, classify with a 47% accuracy.  \n",
    "\n",
    "### Next Steps:\n",
    "1. Supply the entire assessment question, not just the question prompt, to each classifier to help with classification.\n",
    "2. Determine whether the classifier can also identify the \"Essential Knowledge\" assessed by the question, not just the computational skill.\n",
    "3. Attempt to generalize the classifiers to classify non-assessment questions such as lecture material, lab questions, and homework problems.\n",
    "4. Create a visualization that shows the distribution of thinking skills and content assessed over the course of the curriculum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in all questions from multiple documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "document = Document(\"Data/U1Ll, TST-AP Comp Sci A_Unit 1_Week 4_L6_Unit 1 Test.docx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "document.paragraphs[9].text.split(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "text = \"Hi Jeff!  How are you?\"\n",
    "\n",
    "result = re.split(r\"[.!?]\", text)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = re.compile('[A-Za-z ]+[?]')\n",
    "p.findall(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = []\n",
    "\n",
    "for filename in os.listdir(\"Data/\"):\n",
    "  document = Document(\"Data/\" + filename)\n",
    "  for paragraph in document.paragraphs:\n",
    "    result = p.findall(paragraph.text)\n",
    "    if len(result) > 0:\n",
    "      questions.append(result[0])\n",
    "questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in College Board Questions and Areas of Interest/Data Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quickly try to identify classification by taking questions independently - does not work well (30%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "corpus = [\"This is the first document.\", \"This second document is the second document.\", \"And this is the third one.\",\n",
    "          \"Is this the first document?\" ]\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "vectorizer.get_feature_names_out()\n",
    "print(X.toarray())\n",
    "pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Data/CollegeBoard/SamplePrompts-PracticeExam2020.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Classification\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df[\"Prompt\"], df[\"Classification\"])\n",
    "\n",
    "temp2 = pd.DataFrame(zip(X_train, y_train), columns=[\"Prompt\", \"Classification\"])\n",
    "\n",
    "temp = []\n",
    "for question_class in temp2[\"Classification\"].unique():\n",
    "  temp_str = \"\"\n",
    "  for x in df.loc[df.Classification == question_class, \"Prompt\"].values:\n",
    "     temp_str += x\n",
    "  temp.append({\"class\":question_class, \"text\": temp_str})\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(temp)\n",
    "df2\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "X = vectorizer.fit_transform(df2[\"text\"])\n",
    "df2 = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out(), index=df2[\"class\"])\n",
    "#df3 = df3.drop([\"10\", \"12\", \"15\", \"16\", \"987654321\", \"b1\", \"b2\"], axis=1)\n",
    "df2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "lr = LogisticRegression()\n",
    "lr.fit(df2, df2.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.score(df2, df2.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = pd.read_csv(\"Data/CollegeBoard/SamplePrompts-CED.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_vect = vectorizer.transform(df4[\"Prompt\"])\n",
    "X_test_vect = pd.DataFrame(X_test_vect.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "#X_test2_vect = X_test2_vect.drop([\"10\", \"12\", \"15\", \"16\", \"987654321\", \"b1\", \"b2\"], axis=1)\n",
    "\n",
    "y_test = df4[\"Classification\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.score(X_test_vect, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test2_pred = lr.predict(X_test2_vect)\n",
    "y_test2_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test2.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.loc[0, \"Prompt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(y_test2_pred, y_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2014 = pd.read_csv(\"Data/CollegeBoard/SamplePrompts-PracticeExam2014.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = vectorizer.transform(df_2014[\"Prompt\"])\n",
    "X_test = pd.DataFrame(X_test.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "y_test = df_2014[\"Classification\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try training with both 2020 and 2014 questions, and then test on CED questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2014 = pd.read_csv(\"Data/CollegeBoard/SamplePrompts-PracticeExam2014.csv\")\n",
    "#df_2020 = pd.read_csv(\"Data/CollegeBoard/SamplePrompts-PracticeExam2020.csv\")\n",
    "#df = pd.concat([df_2014, df_2020])\n",
    "classifiers_unique = df_2014[\"Classification\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_text = []\n",
    "for x in classifiers_unique:\n",
    "    text_string = \"\"\n",
    "    for y in df.loc[df.Classification == x, \"Prompt\"]:\n",
    "        text_string += y\n",
    "    training_text.append({\"Classification\":x, \"Prompt\":text_string})\n",
    "training_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(training_text)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "X = vectorizer.fit_transform(df[\"Prompt\"])\n",
    "X = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out(), index=df[\"Classification\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X, X.index )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_CED = pd.read_csv(\"Data/CollegeBoard/SamplePrompts-PracticeExam2020.csv\")\n",
    "df_CED=df_CED[~df_CED[\"Classification\"].isin([\"2.D\", \"5.C\", \"5.D\"])]\n",
    "X_test = vectorizer.transform(df_CED[\"Prompt\"])\n",
    "X_test = pd.DataFrame(X_test.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "lr.score(X_test, df_CED[\"Classification\"])\n",
    "y_test_pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(df_CED[\"Classification\"], y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(df_CED[\"Classification\"], y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-4o-mini\",\n",
    "  messages=[{\"role\": \"user\", \"content\": \"Why is Notre Dame football so famous?\"}]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_start = \"Here are the categories for AP questions. \\\n",
    "1.B: Determine code that would be used to complete code segments \\\n",
    "1.C: Determine code that would be used to interact with completed program code. \\\n",
    "2.A: Apply the meaning of specific operators \\\n",
    "2.B: Determine the result or output based on statement execution order in a code segment without method calls (other than output) \\\n",
    "2.C: Determine the result or output based on the statement execution order in a code segment containing method calls. \\\n",
    "2.D: Determine the number of times a code segment will execute. \\\n",
    "4.A: Use test-cases to find errors or validate results. \\\n",
    "4.B: Identify errors in program code. \\\n",
    "4.C: Determine if two or more code segments yield equivalent results. \\\n",
    "5.A: Determine the behavior of a given segment of program code. \\\n",
    "5.B: Explain why a code segment will not compile or work as intended \\\n",
    "5.C: Explain how the result of program code changes, given a change to the initial code. \\\n",
    "5.D: Describe the initial conditions that must be met for a program segment to work as intended or described. \\\n",
    "Which of the categories above best classifies this question prompt below? \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt_guess(prompt):\n",
    "  response = client.chat.completions.create(\n",
    "  model=\"gpt-4o-mini\",\n",
    "  messages=[{\"role\": \"user\", \"content\": prompt_start + prompt}]\n",
    "  )\n",
    "  return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Data/CollegeBoard/SamplePrompts-PracticeExam2020.csv\")\n",
    "\n",
    "for i in df.index:\n",
    "    df.loc[i,\"GPT_Pred\"] = gpt_guess(df.loc[i,\"Prompt\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"Data/output1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df.index:\n",
    "  print(df.loc[i, \"GPT_Pred\"])\n",
    "  code = input(\"What is the code?\")\n",
    "  df.loc[i, \"GPT_Code\"] = code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[0, \"GPT_Code\"] = \"1.B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(df[\"Classification\"], df[\"GPT_Code\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "cm = confusion_matrix(df[\"Classification\"], df[\"GPT_Code\"])\n",
    "(ConfusionMatrixDisplay(cm)).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try to use simplified prompt with work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_start2 = \"Here are the categories for AP questions. \\\n",
    "1.B: Determine code that would be used to complete code segments \\\n",
    "1.C: Determine code that would be used to interact with completed program code. \\\n",
    "2.A: Apply the meaning of specific operators \\\n",
    "2.B: Determine the result or output based on statement execution order in a code segment without method calls (other than output) \\\n",
    "2.C: Determine the result or output based on the statement execution order in a code segment containing method calls. \\\n",
    "2.D: Determine the number of times a code segment will execute. \\\n",
    "4.A: Use test-cases to find errors or validate results. \\\n",
    "4.B: Identify errors in program code. \\\n",
    "4.C: Determine if two or more code segments yield equivalent results. \\\n",
    "5.A: Determine the behavior of a given segment of program code. \\\n",
    "5.B: Explain why a code segment will not compile or work as intended \\\n",
    "5.C: Explain how the result of program code changes, given a change to the initial code. \\\n",
    "5.D: Describe the initial conditions that must be met for a program segment to work as intended or described. \"\n",
    "\n",
    "prompt_question = \"Using the categories previously listed, determine the category for this question prompt:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-4o-mini\",\n",
    "  messages=[{\"role\": \"user\", \"content\": prompt_start2},\n",
    "            {\"role\": \"user\", \"content\": prompt_question+df.loc[0,\"Prompt\"]},\n",
    "            {\"role\": \"user\", \"content\": prompt_question+df.loc[1,\"Prompt\"]},\n",
    "            {\"role\": \"user\", \"content\": prompt_question+df.loc[2,\"Prompt\"]}\n",
    "  ]\n",
    "  )\n",
    "\n",
    "response.choices[1].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Improving TF-IDF by Include Full Question Text in Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Extracting Text from PDF Automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\" \n",
    "    with open(pdf_path, \"rb\") as pdf_file:\n",
    "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "        for page_num in range(len(pdf_reader.pages)):\n",
    "            page = pdf_reader.pages[page_num]\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = \"Data/CollegeBoard/ap-computer-science-a-2014-practice-exam.pdf\"\n",
    "    extract_text = extract_text_from_pdf(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_text = extract_text.replace(\"\\n\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number = 1\n",
    "questions = []\n",
    "\n",
    "for number in range(1,40):\n",
    "  begin = extract_text.find(str(number)+\".\")\n",
    "  end = extract_text.find(str(number+1)+\"2.\")\n",
    "\n",
    "  question = extract_text[begin:end]\n",
    "  option_e = question.find(\"(E)\")\n",
    "  question = question[:option_e]\n",
    "  questions.append({\"number\":number, \"text\":question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_questions = pd.DataFrame(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "df = pd.read_csv(\"Data/CollegeBoard/SamplePrompts-PracticeExam2014.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Question_Num\"] = df[\"Source\"].str.slice(14).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(df_questions, left_on=\"Question_Num\", right_on=\"number\")\n",
    "df.to_csv(\"Data/CollegeBoard/SamplePrompts-PracticeExam2014.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Extracting Text from 2020 Test as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\" \n",
    "    with open(pdf_path, \"rb\") as pdf_file:\n",
    "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "        for page_num in range(len(pdf_reader.pages)):\n",
    "            page = pdf_reader.pages[page_num]\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = \"Data/CollegeBoard/ap-computer-science-a-2020-practice-exam-and-notes-1.pdf\"\n",
    "    extract_text = extract_text_from_pdf(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_text = extract_text.replace(\"\\n\",\"\")\n",
    "\n",
    "questions = []\n",
    "\n",
    "for number in range(1,40):\n",
    "  begin = extract_text.find(str(number)+\".\")\n",
    "  end = extract_text.find(str(number+1)+\"2.\")\n",
    "\n",
    "  question = extract_text[begin:end]\n",
    "  option_e = question.find(\"(E)\")\n",
    "  question = question[:option_e]\n",
    "  questions.append({\"number\":number, \"text\":question})\n",
    "\n",
    "df_questions = pd.DataFrame(questions)\n",
    "\n",
    "df = pd.read_csv(\"Data/CollegeBoard/SamplePrompts-PracticeExam2020.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Question_Num\"] = df[\"Source\"].str.slice(14).astype(int)\n",
    "df = df.merge(df_questions, left_on=\"Question_Num\", right_on=\"number\")\n",
    "df.to_csv(\"Data/CollegeBoard/SamplePrompts-PracticeExam2020.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Training Model on 2020 and Classify 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jeff-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

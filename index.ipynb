{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Curriculum Rigor\n",
    "## Background\n",
    "In my experience with high school curriculum, I have found a wide variation in the rigor of course material.  This project seeks to develop a tool for evaluating the rigor of a curriculum, by measuring its alignment to the College Board's respective AP Course.  This project focuses on the College Board's AP Computer Science A course, which covers a first year Java and Object Orientied Design course.\n",
    "\n",
    "For this course, the College Board defines a set of \"Computational Thinking Practices\" (skills) and content that will be assessed on a year-end summative assessment to determine student's mastery of the course.   \n",
    "\n",
    "There are 5 main Computational Thinking Practices identified by the College Board, which it then breaks down into subskills:  \n",
    "\n",
    "<img src=\"Reports/Images/Skills-List.png\" width=600px> \n",
    "\n",
    "In addition, the College Board defines a set of \"Essential Knowledge\" (the content) to be assessed in the course, which it organizes under 5 \"Big Ideas.\"  For example, the content for a lesson on iteration is: \n",
    "\n",
    "<img src=\"Reports/Images/Content-Sample.png\" width=400px>\n",
    "\n",
    "Every question on the College Board's end-of-course summative exam is aligned to a particular computational thinking skill and essential knowledge.  As a note, some school networks have found the College Board's standards to be very complete, and \"backwards plan\" their middle school and pre-AP high school courses to prepare students for the AP level work. \n",
    "\n",
    "As a first step, this project will focus on the assessment questions used in a particular curriculum, and measure how well they align to the College Board's Computational Thinking Practice and Curriculum Framework.  (As a note, AP classes in most subjects have an analagous set of  thinking practices and framework standards, so one day, this work may be generalized to assess curriculums in other subject areas.)\n",
    "\n",
    "Two questions to assess are:  \n",
    "\n",
    "1. Can a TF-IDF vectorization of College Board question prompt with a Logistic Regressor successfully classify an assessment question by Computational Thinking Practice?\n",
    "2. If ChatGPT is supplied only with the College Board Framework for Computational Thinking, can it successfully identify the particular thinking practice being assessed by a question prompt?\n",
    "\n",
    "### Initial Conclusions:\n",
    "1. When just classifying between two different computational thinking practices, both the Logistic Regression and ChatGPT classify with 100% accuracy\n",
    "\n",
    "### Next Steps:\n",
    "1. Expand this to include all 15 computational thinking practices.  Compare accuracy of Logistic Regression and ChatGPT.\n",
    "2. Determine whether the classifier can also identify the \"Essential Knowledge\" assessed by the question, not just the computational skill.\n",
    "3. Attempt to generalize the classifiers to classify non-assessment questions such as lecture material, lab questions, and homework problems.\n",
    "4. Create a visualization that shows the distribution of thinking skills and content assessed over the course of the curriculum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying Questions Using Logistic Regression\n",
    "As first step, this section will try to classify prompts as assessing one of these two AP Computational Thinking Practices (CTP):\n",
    "1. **CTP 2.A**: Apply the meaning of specific operators.  For example:  \n",
    "*Consider the following code segment.*  \n",
    "```\n",
    "int x = 7;  \n",
    "int y = 3;  \n",
    "if ((x < 10) && (y < 0))  \n",
    "  System.out.println(\"\"Value is: \"\" + x * y);  \n",
    "else  \n",
    "  System.out.println(\"\"Value is: \"\" + x / y) \n",
    "```\n",
    "*What is printed as a result of executing the code segment?*\n",
    "\n",
    "2. **CTP 2.B**: Determine the results or output based on statement execution order in a code segment without method calls (except for output).  For example:  \n",
    "\n",
    "*Consider the following code segment.* \n",
    "```\n",
    "int[] arr = {7, 2, 5, 3, 0, 10};  \n",
    "for (int k = 0; k < arr.length - 1; k++)  {  \n",
    "  if (arr[k] > arr[k + 1])  \n",
    "    System.out.print(k + \"\" \"\" + arr[k] + \"\" \"\");  \n",
    "} \n",
    "``` \n",
    "*What will be printed as a result of executing the code segment?*  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the Data\n",
    "In this first step, we will:\n",
    "1. Read in example prompts for each category.\n",
    "2. Preprocess the data: tokenize, lemmatize, and look at the token frequency distribution by question category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Data/synthetic_all_questions.csv\")\n",
    "df_2014 = pd.read_csv(\"Data/CollegeBoard/SamplePrompts-PracticeExam2014.csv\")\n",
    "df_2014 = df_2014[[\"text\",\"Classification\"]]\n",
    "df_2014.columns=[\"Question\",\"Classification\"]\n",
    "df = pd.concat([df, df_2014])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Prompt\"] = df[\"Question\"].str.extract(\"([A-Z][a-zA-Z /*`]+[?])\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ym/68nrz1n97wj0gz5413bhpqs80000gn/T/ipykernel_74229/4281806620.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"Classification\"] = df[\"Classification\"].str.strip(\" ':\")\n"
     ]
    }
   ],
   "source": [
    "df[\"Classification\"] = df[\"Classification\"].str.strip(\" ':\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"Classification\"].isin([\"1.B\", \"2.A\", \"2.B\", \"2.C\", \"2.D\", \"4.A\", \"5.A\", \"5.B\"])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Logistic Regression Classifier on the Questions\n",
    "Here, we build a pipeline that does the following:\n",
    "1. Preprocess Text: tokenize and lemmatize the text, vectorize using TF-IDF, restricting to 10 features.\n",
    "2. Train and test a logistic regression classifier.  Evaluate the model appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_token_pattern = r\"(?u)\\b\\w\\w+\\b\"\n",
    "\n",
    "tokenizer = RegexpTokenizer(basic_token_pattern)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    temp = tokenizer.tokenize(text.lower())\n",
    "    return [lemmatizer.lemmatize(w) for w in temp]\n",
    "\n",
    "pipeline = [(\"tfidf\",TfidfVectorizer(strip_accents='ascii', tokenizer=lemmatize_text, stop_words=\"english\", max_features=15)),\n",
    "            (\"lr\", LogisticRegression(solver=\"saga\"))]\n",
    "pipe = Pipeline(steps=pipeline)\n",
    "\n",
    "params = [{\"tfidf__max_features\":[5,50,100], \"lr__penalty\":['l1', 'l2'],\"lr__C\":[1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4]}]\n",
    "gs = GridSearchCV(estimator=pipe, param_grid=params)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"Question\"], df[\"Classification\"], stratify=df[\"Classification\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model has a penalty of 'l2' and a C value of 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-10 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-10 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-10 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-10 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-10 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-10 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-10 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-10 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-10 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-10 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-10 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-10 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-10 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-10 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-10 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-10 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-10 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-10\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(estimator=Pipeline(steps=[(&#x27;tfidf&#x27;,\n",
       "                                        TfidfVectorizer(max_features=15,\n",
       "                                                        stop_words=&#x27;english&#x27;,\n",
       "                                                        strip_accents=&#x27;ascii&#x27;,\n",
       "                                                        tokenizer=&lt;function lemmatize_text at 0x302860180&gt;)),\n",
       "                                       (&#x27;lr&#x27;,\n",
       "                                        LogisticRegression(solver=&#x27;saga&#x27;))]),\n",
       "             param_grid=[{&#x27;lr__C&#x27;: [0.001, 0.01, 0.1, 1, 10.0, 100.0, 1000.0,\n",
       "                                    10000.0],\n",
       "                          &#x27;lr__penalty&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;],\n",
       "                          &#x27;tfidf__max_features&#x27;: [5, 50, 100]}])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-30\" type=\"checkbox\" ><label for=\"sk-estimator-id-30\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;GridSearchCV<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.model_selection.GridSearchCV.html\">?<span>Documentation for GridSearchCV</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>GridSearchCV(estimator=Pipeline(steps=[(&#x27;tfidf&#x27;,\n",
       "                                        TfidfVectorizer(max_features=15,\n",
       "                                                        stop_words=&#x27;english&#x27;,\n",
       "                                                        strip_accents=&#x27;ascii&#x27;,\n",
       "                                                        tokenizer=&lt;function lemmatize_text at 0x302860180&gt;)),\n",
       "                                       (&#x27;lr&#x27;,\n",
       "                                        LogisticRegression(solver=&#x27;saga&#x27;))]),\n",
       "             param_grid=[{&#x27;lr__C&#x27;: [0.001, 0.01, 0.1, 1, 10.0, 100.0, 1000.0,\n",
       "                                    10000.0],\n",
       "                          &#x27;lr__penalty&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;],\n",
       "                          &#x27;tfidf__max_features&#x27;: [5, 50, 100]}])</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-31\" type=\"checkbox\" ><label for=\"sk-estimator-id-31\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">best_estimator_: Pipeline</label><div class=\"sk-toggleable__content fitted\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;,\n",
       "                 TfidfVectorizer(max_features=50, stop_words=&#x27;english&#x27;,\n",
       "                                 strip_accents=&#x27;ascii&#x27;,\n",
       "                                 tokenizer=&lt;function lemmatize_text at 0x302860180&gt;)),\n",
       "                (&#x27;lr&#x27;,\n",
       "                 LogisticRegression(C=10.0, penalty=&#x27;l1&#x27;, solver=&#x27;saga&#x27;))])</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-32\" type=\"checkbox\" ><label for=\"sk-estimator-id-32\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;TfidfVectorizer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\">?<span>Documentation for TfidfVectorizer</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>TfidfVectorizer(max_features=50, stop_words=&#x27;english&#x27;, strip_accents=&#x27;ascii&#x27;,\n",
       "                tokenizer=&lt;function lemmatize_text at 0x302860180&gt;)</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-33\" type=\"checkbox\" ><label for=\"sk-estimator-id-33\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;LogisticRegression<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression(C=10.0, penalty=&#x27;l1&#x27;, solver=&#x27;saga&#x27;)</pre></div> </div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(estimator=Pipeline(steps=[('tfidf',\n",
       "                                        TfidfVectorizer(max_features=15,\n",
       "                                                        stop_words='english',\n",
       "                                                        strip_accents='ascii',\n",
       "                                                        tokenizer=<function lemmatize_text at 0x302860180>)),\n",
       "                                       ('lr',\n",
       "                                        LogisticRegression(solver='saga'))]),\n",
       "             param_grid=[{'lr__C': [0.001, 0.01, 0.1, 1, 10.0, 100.0, 1000.0,\n",
       "                                    10000.0],\n",
       "                          'lr__penalty': ['l1', 'l2'],\n",
       "                          'tfidf__max_features': [5, 50, 100]}])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.01635523, 0.00973969, 0.00928087, 0.01087694, 0.01154523,\n",
       "        0.01209788, 0.00959663, 0.00913348, 0.00902433, 0.01022034,\n",
       "        0.01091833, 0.01119814, 0.00898147, 0.00929279, 0.00902605,\n",
       "        0.01000056, 0.01095362, 0.01124368, 0.01069589, 0.01225653,\n",
       "        0.01283803, 0.01069427, 0.01145267, 0.01176929, 0.01095843,\n",
       "        0.01523376, 0.01567955, 0.01161132, 0.0138392 , 0.01446538,\n",
       "        0.01184239, 0.01844182, 0.02082095, 0.01107807, 0.01183505,\n",
       "        0.01188107, 0.01147227, 0.02186093, 0.03117695, 0.01271124,\n",
       "        0.01387491, 0.01374493, 0.01320558, 0.02662835, 0.04331489,\n",
       "        0.01301455, 0.01274571, 0.01376724]),\n",
       " 'std_fit_time': array([6.88905988e-03, 4.64572520e-04, 6.07385357e-05, 4.47615130e-04,\n",
       "        7.15062074e-04, 1.51975521e-04, 3.77921994e-04, 2.09152410e-04,\n",
       "        1.23710105e-04, 1.61694869e-04, 2.43263438e-04, 2.05752587e-04,\n",
       "        1.14653608e-04, 2.91713892e-04, 1.32363760e-04, 9.78184216e-05,\n",
       "        3.56891363e-04, 3.23390054e-04, 3.09449880e-04, 2.05960348e-04,\n",
       "        2.81696918e-04, 2.12506105e-04, 9.28108133e-05, 2.98578241e-04,\n",
       "        2.94600130e-04, 5.88391994e-04, 3.17875737e-04, 2.24508971e-04,\n",
       "        1.11173419e-03, 1.13951203e-03, 4.17839225e-04, 6.56480708e-04,\n",
       "        1.18382780e-03, 2.49677937e-04, 4.71516619e-04, 1.84419223e-04,\n",
       "        5.95772364e-04, 3.17780060e-04, 1.25225034e-03, 5.32510240e-04,\n",
       "        2.74106165e-04, 7.85758634e-04, 3.41089835e-04, 1.67605177e-03,\n",
       "        9.30356115e-04, 5.54838116e-04, 2.36735151e-04, 1.64376358e-04]),\n",
       " 'mean_score_time': array([0.00287385, 0.00222898, 0.0022696 , 0.00248604, 0.00235677,\n",
       "        0.00244355, 0.00227656, 0.00218372, 0.00220175, 0.00224099,\n",
       "        0.00218673, 0.00215902, 0.00213919, 0.00225124, 0.00214882,\n",
       "        0.00219064, 0.00223408, 0.00220819, 0.00224266, 0.00238905,\n",
       "        0.00234957, 0.0022872 , 0.00233674, 0.00228629, 0.00234432,\n",
       "        0.00305901, 0.00266919, 0.00268097, 0.00265074, 0.00294623,\n",
       "        0.0024343 , 0.00294485, 0.00275807, 0.00240612, 0.00231743,\n",
       "        0.00233855, 0.00242658, 0.00242939, 0.00266871, 0.00263009,\n",
       "        0.00254097, 0.0029254 , 0.00266204, 0.00273933, 0.00292172,\n",
       "        0.00261345, 0.00259242, 0.00270638]),\n",
       " 'std_score_time': array([5.94326100e-04, 1.04227375e-04, 1.48635168e-04, 1.52537472e-04,\n",
       "        2.00577854e-04, 2.23089527e-04, 1.57684259e-04, 1.12764316e-04,\n",
       "        1.23332564e-04, 1.73673991e-04, 1.26925543e-04, 1.15864620e-04,\n",
       "        1.23177255e-04, 7.30187951e-05, 1.11666507e-04, 1.41649725e-04,\n",
       "        2.22851089e-04, 1.26712905e-04, 2.24423731e-04, 2.34900583e-04,\n",
       "        1.37698931e-04, 1.29740933e-04, 1.12858501e-04, 1.59424968e-04,\n",
       "        3.11591772e-04, 4.21288762e-04, 4.61149943e-04, 3.24255978e-04,\n",
       "        1.10087336e-04, 7.14539433e-04, 2.37535242e-04, 4.37668294e-04,\n",
       "        2.72832680e-04, 1.87078052e-04, 9.80202542e-05, 1.77016867e-04,\n",
       "        2.73972780e-04, 4.85703998e-05, 3.94599923e-04, 2.21564098e-04,\n",
       "        1.37529095e-04, 3.23403918e-04, 1.74965037e-04, 1.25081903e-04,\n",
       "        3.46561973e-04, 2.43006742e-04, 1.01786875e-04, 2.91820936e-04]),\n",
       " 'param_lr__C': masked_array(data=[0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 10.0, 10.0, 10.0, 10.0,\n",
       "                    10.0, 10.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0,\n",
       "                    1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0,\n",
       "                    10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value=1e+20),\n",
       " 'param_lr__penalty': masked_array(data=['l1', 'l1', 'l1', 'l2', 'l2', 'l2', 'l1', 'l1', 'l1',\n",
       "                    'l2', 'l2', 'l2', 'l1', 'l1', 'l1', 'l2', 'l2', 'l2',\n",
       "                    'l1', 'l1', 'l1', 'l2', 'l2', 'l2', 'l1', 'l1', 'l1',\n",
       "                    'l2', 'l2', 'l2', 'l1', 'l1', 'l1', 'l2', 'l2', 'l2',\n",
       "                    'l1', 'l1', 'l1', 'l2', 'l2', 'l2', 'l1', 'l1', 'l1',\n",
       "                    'l2', 'l2', 'l2'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_tfidf__max_features': masked_array(data=[5, 50, 100, 5, 50, 100, 5, 50, 100, 5, 50, 100, 5, 50,\n",
       "                    100, 5, 50, 100, 5, 50, 100, 5, 50, 100, 5, 50, 100, 5,\n",
       "                    50, 100, 5, 50, 100, 5, 50, 100, 5, 50, 100, 5, 50,\n",
       "                    100, 5, 50, 100, 5, 50, 100],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value=999999),\n",
       " 'params': [{'lr__C': 0.001, 'lr__penalty': 'l1', 'tfidf__max_features': 5},\n",
       "  {'lr__C': 0.001, 'lr__penalty': 'l1', 'tfidf__max_features': 50},\n",
       "  {'lr__C': 0.001, 'lr__penalty': 'l1', 'tfidf__max_features': 100},\n",
       "  {'lr__C': 0.001, 'lr__penalty': 'l2', 'tfidf__max_features': 5},\n",
       "  {'lr__C': 0.001, 'lr__penalty': 'l2', 'tfidf__max_features': 50},\n",
       "  {'lr__C': 0.001, 'lr__penalty': 'l2', 'tfidf__max_features': 100},\n",
       "  {'lr__C': 0.01, 'lr__penalty': 'l1', 'tfidf__max_features': 5},\n",
       "  {'lr__C': 0.01, 'lr__penalty': 'l1', 'tfidf__max_features': 50},\n",
       "  {'lr__C': 0.01, 'lr__penalty': 'l1', 'tfidf__max_features': 100},\n",
       "  {'lr__C': 0.01, 'lr__penalty': 'l2', 'tfidf__max_features': 5},\n",
       "  {'lr__C': 0.01, 'lr__penalty': 'l2', 'tfidf__max_features': 50},\n",
       "  {'lr__C': 0.01, 'lr__penalty': 'l2', 'tfidf__max_features': 100},\n",
       "  {'lr__C': 0.1, 'lr__penalty': 'l1', 'tfidf__max_features': 5},\n",
       "  {'lr__C': 0.1, 'lr__penalty': 'l1', 'tfidf__max_features': 50},\n",
       "  {'lr__C': 0.1, 'lr__penalty': 'l1', 'tfidf__max_features': 100},\n",
       "  {'lr__C': 0.1, 'lr__penalty': 'l2', 'tfidf__max_features': 5},\n",
       "  {'lr__C': 0.1, 'lr__penalty': 'l2', 'tfidf__max_features': 50},\n",
       "  {'lr__C': 0.1, 'lr__penalty': 'l2', 'tfidf__max_features': 100},\n",
       "  {'lr__C': 1, 'lr__penalty': 'l1', 'tfidf__max_features': 5},\n",
       "  {'lr__C': 1, 'lr__penalty': 'l1', 'tfidf__max_features': 50},\n",
       "  {'lr__C': 1, 'lr__penalty': 'l1', 'tfidf__max_features': 100},\n",
       "  {'lr__C': 1, 'lr__penalty': 'l2', 'tfidf__max_features': 5},\n",
       "  {'lr__C': 1, 'lr__penalty': 'l2', 'tfidf__max_features': 50},\n",
       "  {'lr__C': 1, 'lr__penalty': 'l2', 'tfidf__max_features': 100},\n",
       "  {'lr__C': 10.0, 'lr__penalty': 'l1', 'tfidf__max_features': 5},\n",
       "  {'lr__C': 10.0, 'lr__penalty': 'l1', 'tfidf__max_features': 50},\n",
       "  {'lr__C': 10.0, 'lr__penalty': 'l1', 'tfidf__max_features': 100},\n",
       "  {'lr__C': 10.0, 'lr__penalty': 'l2', 'tfidf__max_features': 5},\n",
       "  {'lr__C': 10.0, 'lr__penalty': 'l2', 'tfidf__max_features': 50},\n",
       "  {'lr__C': 10.0, 'lr__penalty': 'l2', 'tfidf__max_features': 100},\n",
       "  {'lr__C': 100.0, 'lr__penalty': 'l1', 'tfidf__max_features': 5},\n",
       "  {'lr__C': 100.0, 'lr__penalty': 'l1', 'tfidf__max_features': 50},\n",
       "  {'lr__C': 100.0, 'lr__penalty': 'l1', 'tfidf__max_features': 100},\n",
       "  {'lr__C': 100.0, 'lr__penalty': 'l2', 'tfidf__max_features': 5},\n",
       "  {'lr__C': 100.0, 'lr__penalty': 'l2', 'tfidf__max_features': 50},\n",
       "  {'lr__C': 100.0, 'lr__penalty': 'l2', 'tfidf__max_features': 100},\n",
       "  {'lr__C': 1000.0, 'lr__penalty': 'l1', 'tfidf__max_features': 5},\n",
       "  {'lr__C': 1000.0, 'lr__penalty': 'l1', 'tfidf__max_features': 50},\n",
       "  {'lr__C': 1000.0, 'lr__penalty': 'l1', 'tfidf__max_features': 100},\n",
       "  {'lr__C': 1000.0, 'lr__penalty': 'l2', 'tfidf__max_features': 5},\n",
       "  {'lr__C': 1000.0, 'lr__penalty': 'l2', 'tfidf__max_features': 50},\n",
       "  {'lr__C': 1000.0, 'lr__penalty': 'l2', 'tfidf__max_features': 100},\n",
       "  {'lr__C': 10000.0, 'lr__penalty': 'l1', 'tfidf__max_features': 5},\n",
       "  {'lr__C': 10000.0, 'lr__penalty': 'l1', 'tfidf__max_features': 50},\n",
       "  {'lr__C': 10000.0, 'lr__penalty': 'l1', 'tfidf__max_features': 100},\n",
       "  {'lr__C': 10000.0, 'lr__penalty': 'l2', 'tfidf__max_features': 5},\n",
       "  {'lr__C': 10000.0, 'lr__penalty': 'l2', 'tfidf__max_features': 50},\n",
       "  {'lr__C': 10000.0, 'lr__penalty': 'l2', 'tfidf__max_features': 100}],\n",
       " 'split0_test_score': array([0.125 , 0.0625, 0.125 , 0.125 , 0.125 , 0.1875, 0.125 , 0.125 ,\n",
       "        0.0625, 0.1875, 0.1875, 0.3125, 0.125 , 0.125 , 0.1875, 0.125 ,\n",
       "        0.4375, 0.4375, 0.125 , 0.6875, 0.5625, 0.1875, 0.6875, 0.75  ,\n",
       "        0.1875, 0.75  , 0.625 , 0.125 , 0.6875, 0.6875, 0.1875, 0.6875,\n",
       "        0.6875, 0.1875, 0.6875, 0.6875, 0.1875, 0.6875, 0.6875, 0.1875,\n",
       "        0.75  , 0.6875, 0.1875, 0.6875, 0.6875, 0.1875, 0.6875, 0.625 ]),\n",
       " 'split1_test_score': array([0.13333333, 0.13333333, 0.13333333, 0.13333333, 0.2       ,\n",
       "        0.2       , 0.2       , 0.13333333, 0.13333333, 0.13333333,\n",
       "        0.33333333, 0.2       , 0.13333333, 0.2       , 0.2       ,\n",
       "        0.13333333, 0.4       , 0.26666667, 0.13333333, 0.66666667,\n",
       "        0.6       , 0.26666667, 0.93333333, 0.8       , 0.33333333,\n",
       "        0.93333333, 1.        , 0.33333333, 0.93333333, 0.8       ,\n",
       "        0.33333333, 0.93333333, 0.93333333, 0.33333333, 0.93333333,\n",
       "        0.8       , 0.33333333, 0.93333333, 0.8       , 0.33333333,\n",
       "        0.93333333, 0.8       , 0.33333333, 0.93333333, 0.8       ,\n",
       "        0.33333333, 0.93333333, 0.8       ]),\n",
       " 'split2_test_score': array([0.13333333, 0.13333333, 0.13333333, 0.13333333, 0.13333333,\n",
       "        0.13333333, 0.13333333, 0.13333333, 0.2       , 0.13333333,\n",
       "        0.2       , 0.13333333, 0.2       , 0.13333333, 0.13333333,\n",
       "        0.13333333, 0.46666667, 0.4       , 0.2       , 0.6       ,\n",
       "        0.6       , 0.26666667, 0.66666667, 0.66666667, 0.33333333,\n",
       "        0.8       , 0.6       , 0.33333333, 0.73333333, 0.6       ,\n",
       "        0.33333333, 0.66666667, 0.66666667, 0.33333333, 0.73333333,\n",
       "        0.6       , 0.33333333, 0.73333333, 0.6       , 0.33333333,\n",
       "        0.73333333, 0.6       , 0.33333333, 0.73333333, 0.6       ,\n",
       "        0.33333333, 0.73333333, 0.6       ]),\n",
       " 'split3_test_score': array([0.13333333, 0.13333333, 0.2       , 0.13333333, 0.13333333,\n",
       "        0.13333333, 0.06666667, 0.13333333, 0.13333333, 0.13333333,\n",
       "        0.13333333, 0.13333333, 0.2       , 0.13333333, 0.13333333,\n",
       "        0.13333333, 0.4       , 0.2       , 0.26666667, 0.53333333,\n",
       "        0.53333333, 0.2       , 0.46666667, 0.4       , 0.26666667,\n",
       "        0.6       , 0.53333333, 0.26666667, 0.53333333, 0.46666667,\n",
       "        0.26666667, 0.46666667, 0.6       , 0.26666667, 0.53333333,\n",
       "        0.46666667, 0.26666667, 0.53333333, 0.6       , 0.26666667,\n",
       "        0.53333333, 0.53333333, 0.26666667, 0.53333333, 0.53333333,\n",
       "        0.26666667, 0.53333333, 0.53333333]),\n",
       " 'split4_test_score': array([0.13333333, 0.06666667, 0.2       , 0.13333333, 0.13333333,\n",
       "        0.13333333, 0.13333333, 0.13333333, 0.13333333, 0.13333333,\n",
       "        0.13333333, 0.13333333, 0.2       , 0.13333333, 0.06666667,\n",
       "        0.13333333, 0.26666667, 0.26666667, 0.2       , 0.6       ,\n",
       "        0.4       , 0.4       , 0.73333333, 0.8       , 0.53333333,\n",
       "        0.53333333, 0.66666667, 0.6       , 0.66666667, 0.73333333,\n",
       "        0.53333333, 0.66666667, 0.66666667, 0.53333333, 0.66666667,\n",
       "        0.73333333, 0.53333333, 0.66666667, 0.73333333, 0.53333333,\n",
       "        0.66666667, 0.73333333, 0.53333333, 0.66666667, 0.73333333,\n",
       "        0.53333333, 0.66666667, 0.73333333]),\n",
       " 'mean_test_score': array([0.13166667, 0.10583333, 0.15833333, 0.13166667, 0.145     ,\n",
       "        0.1575    , 0.13166667, 0.13166667, 0.1325    , 0.14416667,\n",
       "        0.1975    , 0.1825    , 0.17166667, 0.145     , 0.14416667,\n",
       "        0.13166667, 0.39416667, 0.31416667, 0.185     , 0.6175    ,\n",
       "        0.53916667, 0.26416667, 0.6975    , 0.68333333, 0.33083333,\n",
       "        0.72333333, 0.685     , 0.33166667, 0.71083333, 0.6575    ,\n",
       "        0.33083333, 0.68416667, 0.71083333, 0.33083333, 0.71083333,\n",
       "        0.6575    , 0.33083333, 0.71083333, 0.68416667, 0.33083333,\n",
       "        0.72333333, 0.67083333, 0.33083333, 0.71083333, 0.67083333,\n",
       "        0.33083333, 0.71083333, 0.65833333]),\n",
       " 'std_test_score': array([0.00333333, 0.03370625, 0.0341565 , 0.00333333, 0.02768875,\n",
       "        0.02986079, 0.04229526, 0.00333333, 0.04349329, 0.02166667,\n",
       "        0.07320064, 0.06994045, 0.03480102, 0.02768875, 0.04740488,\n",
       "        0.00333333, 0.06849574, 0.08958236, 0.05174725, 0.05479761,\n",
       "        0.07395569, 0.07544314, 0.14915503, 0.1498147 , 0.11461287,\n",
       "        0.14282857, 0.16333333, 0.15423647, 0.12962553, 0.11548208,\n",
       "        0.11461287, 0.14833333, 0.11509658, 0.11461287, 0.12962553,\n",
       "        0.11548208, 0.11461287, 0.12962553, 0.0774776 , 0.11461287,\n",
       "        0.12978615, 0.09464847, 0.11461287, 0.12962553, 0.09464847,\n",
       "        0.11461287, 0.12962553, 0.09574271]),\n",
       " 'rank_test_score': array([43, 48, 36, 43, 38, 37, 43, 43, 42, 41, 32, 34, 35, 38, 40, 43, 21,\n",
       "        30, 33, 19, 20, 31,  9, 13, 23,  1, 10, 22,  4, 17, 23, 11,  3, 23,\n",
       "         4, 17, 23,  4, 11, 23,  2, 14, 23,  4, 14, 23,  4, 16],\n",
       "       dtype=int32)}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.B       1.00      0.60      0.75         5\n",
      "         2.A       0.67      0.67      0.67         3\n",
      "         2.B       0.67      1.00      0.80         4\n",
      "         2.C       0.50      0.25      0.33         4\n",
      "         2.D       1.00      1.00      1.00         2\n",
      "         4.A       0.00      0.00      0.00         2\n",
      "         5.A       0.67      0.67      0.67         3\n",
      "         5.B       1.00      1.00      1.00         3\n",
      "\n",
      "    accuracy                           0.65        26\n",
      "   macro avg       0.69      0.65      0.65        26\n",
      "weighted avg       0.72      0.65      0.66        26\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = gs.predict(X_test)\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "con_mat = confusion_matrix(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = ConfusionMatrixDisplay(con_mat).plot()\n",
    "cm.figure_.savefig(\"Reports/Images/LR_ConfusionMatrix.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the Logistic Classifier to Identify Key Features\n",
    "Determine the most significant works for identifying each question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('return', 8.676627468966188),\n",
       " ('print', 6.086456047084747),\n",
       " ('length', 3.9371922125570586),\n",
       " ('missing', 3.643146889896172),\n",
       " ('double', 2.9982043263413205),\n",
       " ('odd', 2.7817795248629844),\n",
       " ('intended', 2.2731467214745638),\n",
       " ('code', 2.0001183962298392),\n",
       " ('val', 1.5316095864689772),\n",
       " ('sum', 1.0694421391758118),\n",
       " ('array', 0.8344333805072368),\n",
       " ('data', 0.011479195385385669),\n",
       " ('10', 0.0),\n",
       " ('15', 0.0),\n",
       " ('20', 0.0),\n",
       " ('add', 0.0),\n",
       " ('arraylist', 0.0),\n",
       " ('book', 0.0),\n",
       " ('class', 0.0),\n",
       " ('consider', 0.0),\n",
       " ('doe', 0.0),\n",
       " ('element', 0.0),\n",
       " ('following', 0.0),\n",
       " ('int', 0.0),\n",
       " ('integer', 0.0),\n",
       " ('java', 0.0),\n",
       " ('line', 0.0),\n",
       " ('list', 0.0),\n",
       " ('method', 0.0),\n",
       " ('new', 0.0),\n",
       " ('num', 0.0),\n",
       " ('numlist', 0.0),\n",
       " ('nums', 0.0),\n",
       " ('output', 0.0),\n",
       " ('printed', 0.0),\n",
       " ('println', 0.0),\n",
       " ('public', 0.0),\n",
       " ('remove', 0.0),\n",
       " ('result', 0.0),\n",
       " ('segment', 0.0),\n",
       " ('size', 0.0),\n",
       " ('static', 0.0),\n",
       " ('true', 0.0),\n",
       " ('value', 0.0),\n",
       " ('variable', 0.0),\n",
       " ('count', -0.12947627919241045),\n",
       " ('string', -0.25153073119315617),\n",
       " ('void', -1.0228701696866516),\n",
       " ('arr', -1.9494322294722797),\n",
       " ('executed', -2.677416238339786)]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc = {}\n",
    "for x in gs.best_estimator_[\"tfidf\"].vocabulary_:\n",
    "    voc[gs.best_estimator_[\"tfidf\"].vocabulary_[x]] = x\n",
    "voc={k: v for k, v in sorted(gs.best_estimator_[\"tfidf\"].vocabulary_.items(), key=lambda item: item[1])}\n",
    "#pipe[\"lr\"].coef_[0]\n",
    "\n",
    "key_features = list(zip(list(voc.keys()), gs.best_estimator_[\"lr\"].coef_[0]))\n",
    "\n",
    "key_features.sort(key=lambda x: x[1], reverse=True);\n",
    "key_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try with Sentence Transformer Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'cached_download' (from 'huggingface_hub.file_download') is deprecated and will be removed from version '0.26'. Use `hf_hub_download` instead.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2d2772de65c41e48e19e1e7eb9aa40e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/1.23k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d09e782e2b1e4f2faa13a29359d1022c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bb869dceb6c4bbfab05913d6982a6e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2fff3fa1ad949e78fca6a5a3ed360ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38008f7dbb494c53a6e4f03492ae8afb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4371f8c817f44dddb7c34d6323697562",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data_config.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da12b1b545a647569232a430049de751",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f5621048b9e46359dd0d3bedc0f19bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.onnx:   0%|          | 0.00/90.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6de8ae043d843e2b38cca2020cd3750",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_O1.onnx:   0%|          | 0.00/90.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24f0ffba684941878bfb15e9d84b4b52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_O2.onnx:   0%|          | 0.00/90.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a76151cfbe0843889ed34c2fc1985230",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_O3.onnx:   0%|          | 0.00/90.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc7918279c2e456da03e7c680b3d6e3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_O4.onnx:   0%|          | 0.00/45.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bb2a17f49064d26bbee996622e6014b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_qint8_arm64.onnx:   0%|          | 0.00/23.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d22aeddd17884b58a210bcaa851d041f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_qint8_avx512.onnx:   0%|          | 0.00/23.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be06799afd9940daa82610bd0d32de63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_qint8_avx512_vnni.onnx:   0%|          | 0.00/23.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05ff76c52bf0431abee0ca03e21c37ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_quint8_avx2.onnx:   0%|          | 0.00/23.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cced5573755c4fefafe68d6f943308ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "openvino_model.bin:   0%|          | 0.00/90.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d76b912012a640a9a324c4624813540d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "openvino/openvino_model.xml:   0%|          | 0.00/211k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "464e5379eef34642988d4712ac1fb56d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "openvino_model_qint8_quantized.bin:   0%|          | 0.00/22.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "489646b42b9e4bedb3f3912884471969",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)nvino/openvino_model_qint8_quantized.xml:   0%|          | 0.00/368k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99a8ce761f484a90b68da424b1cb5f1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2169dc530650429cb8c1e12b89e66d8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a2c8c3cc9fa497d8263c872ceb5faf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b0d5174cb724f468861053d010a719d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c55114c4f0c941fbac884e1e3eacd40f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3abcf7b3260249e8bc0a7d6584a303b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train_script.py:   0%|          | 0.00/13.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccaeaf831cc44071a2d7c0609d583d63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7e14ae83f33487ebb9ca972908f9489",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_encode = model.encode(X_train.to_numpy())\n",
    "X_test_encode = model.encode(X_test.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-14 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-14 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-14 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-14 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-14 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-14 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-14 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-14 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-14 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-14 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-14 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-14 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-14 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-14 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-14 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-14 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-14 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-14 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-14 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-14 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-14 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-14 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-14 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-14 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-14 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-14 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-14 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-14 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-14 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-14 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-14 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-14 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-14 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-14 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-14 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-14 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-14 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-14 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-14 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-14 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-14 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-14 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-14\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(estimator=LogisticRegression(max_iter=10000, solver=&#x27;saga&#x27;),\n",
       "             param_grid=[{&#x27;C&#x27;: [0.001, 0.01, 0.1, 1, 10.0, 100.0, 1000.0,\n",
       "                                10000.0],\n",
       "                          &#x27;penalty&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;]}])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-43\" type=\"checkbox\" ><label for=\"sk-estimator-id-43\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;GridSearchCV<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.model_selection.GridSearchCV.html\">?<span>Documentation for GridSearchCV</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>GridSearchCV(estimator=LogisticRegression(max_iter=10000, solver=&#x27;saga&#x27;),\n",
       "             param_grid=[{&#x27;C&#x27;: [0.001, 0.01, 0.1, 1, 10.0, 100.0, 1000.0,\n",
       "                                10000.0],\n",
       "                          &#x27;penalty&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;]}])</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-44\" type=\"checkbox\" ><label for=\"sk-estimator-id-44\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">best_estimator_: LogisticRegression</label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression(C=100.0, max_iter=10000, penalty=&#x27;l1&#x27;, solver=&#x27;saga&#x27;)</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-45\" type=\"checkbox\" ><label for=\"sk-estimator-id-45\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;LogisticRegression<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression(C=100.0, max_iter=10000, penalty=&#x27;l1&#x27;, solver=&#x27;saga&#x27;)</pre></div> </div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(estimator=LogisticRegression(max_iter=10000, solver='saga'),\n",
       "             param_grid=[{'C': [0.001, 0.01, 0.1, 1, 10.0, 100.0, 1000.0,\n",
       "                                10000.0],\n",
       "                          'penalty': ['l1', 'l2']}])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(solver='saga', max_iter=10000)\n",
    "params = [{\"penalty\":['l1','l2'], 'C':[1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4]}]\n",
    "search = GridSearchCV(estimator=lr, param_grid=params)\n",
    "\n",
    "search.fit(X_train_encode, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = search.predict(X_test_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 0, 0, 1, 0, 1, 0, 0],\n",
       "       [0, 2, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 4, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 2, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 2, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 2, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 3, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 3]])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.B       0.75      0.60      0.67         5\n",
      "         2.A       1.00      0.67      0.80         3\n",
      "         2.B       0.80      1.00      0.89         4\n",
      "         2.C       0.67      0.50      0.57         4\n",
      "         2.D       1.00      1.00      1.00         2\n",
      "         4.A       0.00      0.00      0.00         2\n",
      "         5.A       0.60      1.00      0.75         3\n",
      "         5.B       1.00      1.00      1.00         3\n",
      "\n",
      "    accuracy                           0.73        26\n",
      "   macro avg       0.73      0.72      0.71        26\n",
      "weighted avg       0.75      0.73      0.72        26\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('4.A', 0.48883927),\n",
       " ('2.A', 0.2824875),\n",
       " ('5.A', 0.10931208),\n",
       " ('1.B', 0.042056117),\n",
       " ('2.B', 0.028736034),\n",
       " ('2.D', 0.026581297),\n",
       " ('5.B', 0.01682231),\n",
       " ('2.C', 0.0051653753)]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_question = \"\"\" \n",
    "What will be printed when the following code is run?  int i=5; if (i/2>3) System.out.println(i%6) else System.out.println(i);\n",
    "\"\"\"\n",
    "results = search.predict_proba([model.encode(my_question)])\n",
    "results = list(zip(search.best_estimator_.classes_, results[0]))\n",
    "results.sort(key=lambda x:x[1], reverse=True)\n",
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Test a ChatGPT Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This classifier asks ChatGPT to determine the Computational Thinking Skill being assessed in the problem.  \n",
    "1.  Create a function call to ask ChatGPT for the classification\n",
    "2.  Test ChatGPT on data set and evaluate classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_start = \"\"\"\n",
    "Here are the categories for AP questions. \n",
    "1.B: Determine code that would be used to complete code segments \n",
    "1.C: Determine code that would be used to interact with completed program code. \n",
    "2.A: Apply the meaning of specific operators \n",
    "2.B: Determine the result or output based on statement execution order in a code segment without method calls (other than output) \n",
    "2.C: Determine the result or output based on the statement execution order in a code segment containing method calls. \n",
    "2.D: Determine the number of times a code segment will execute. \n",
    "4.A: Use test-cases to find errors or validate results. \n",
    "4.B: Identify errors in program code. \n",
    "4.C: Determine if two or more code segments yield equivalent results. \n",
    "5.A: Determine the behavior of a given segment of program code. \n",
    "5.B: Explain why a code segment will not compile or work as intended \n",
    "5.C: Explain how the result of program code changes, given a change to the initial code. \n",
    "5.D: Describe the initial conditions that must be met for a program segment to work as intended or described. \n",
    "\n",
    "Below are three example questions for each category \n",
    "1.B: Consider  the mode  method,  which  is intended  to return  the most  frequently  occurring  value  (mode ) in its int[]  parameter  arr. For example,  if the parameter  of the mode  method  has the contents  {6,5, 1, 5, 2, 6, 5}, then the method  is intended  to return  5. /** Precondition: arr.length  >= 1 */ public  static  int mode(int[]  arr)  { int modeCount  = 1; int mode  = arr[0];  for (int  j = 0; j < arr.length;  j++)  { int valCount  = 0; for (int  k = 0; k < arr.length;  k++)  { if ( /* missing  condition  1 */ ) { valCount++;  } } if ( /* missing  condition  2 */ ) { modeCount  = valCount;  mode  = arr[j];  } } return  mode;  } Which  of the following  can replace  /* missing  condition  1 */ and /* missing  condition  2 */ so the code  segment  works  as intended?  /* missing  condition  1 */ /* missing  condition  2 */ (A) arr[j]  == arr[k]  valCount  > modeCount  (B) arr[j]  == arr[k]  modeCount  > valCount  (C) arr[j]  != arr[k]  valCount  > modeCount  (D) arr[j]  != arr[k]  modeCount  > valCount \n",
    "1.B: Which  of the following  code segments  produces  the output  \"987654321 \" ? (A) int num = 10; while  (num  > 0) { System.out.print(num);  num--;  } (B) int num = 10; while  (num  >= 0) { System.out.print(num);  num--;  } (C) int num = 10; while  (num  > 1) { num--;  System.out.print(num);  } (D) int num = 10; while  (num  >= 1) { num--;  System.out.print(num);  } \n",
    "1.B: Consider  the following  method   which  is intended  to print  the values  in its two-dimensional  integer  array  parameter  in row-major  order.  public  static  void  rowMajor(int[][]  arr)  { /* missing  code  */ } As an example   consider  the following  code  segment.  int[][]  theArray  = {{1   2}  {3  4}  {5  6}  {7  8}};  rowMajor(theArray);  When  executed   the code  segment  should  produce  the following  output.  12345678  GO ON TO THE NEXT  PAGE.  AP Computer Science A Practice Exam 33 Which  of the following  code segments  can replace  /* missing  code  */ so that the rowMajor  method  works  as intended?  (A) for (int  j : arr)  { for (int  k : j) { System.out.print(j  + \"\"); } } (B) for (int  j : arr)  { for (int  k : j) { System.out.print(k  + \"\"); } } (C) for (int[]  j : arr)  { for (int  k : j) { System.out.print(j  + \"\"); } } (D) for (int[]  j : arr)  { for (int  k : j) { System.out.print(k  + \"\"); } } \n",
    "1.C: Consider  the following  class  definition.  public  class  Value  { private  int num;  public  int getNum()  { return  num;  } // There  may be instance  variables,  constructors,  and methods  not shown.  } The following  method  appears  in a class  other  than Value . It is intended  to sum all the num instance  variables  of the Value  objects  in its ArrayList  parameter.  /** Precondition: valueList  is not null  */ public  static  int getTotal(ArrayList<Value>  valueList)  { int total  = 0; /* missing  code  */ return  total;  } Which  of the following  code  segments  can replace  /* missing  code  */ so the getTotal  method  works  as intended?  I. for (int  x = 0; x < valueList.size();  x++)  { total  += valueList.get(x).getNum();  } II. for (Value  v : valueList)  { total  += v.getNum();  } III. for (Value  v : valueList)  { total  += getNum(v);  } (A) I only (B) II only (C) III only (D) I and II\n",
    "1.C: Which of the following statements assigns a random integer between 25 and 60, inclusively, to rn? (A) int rn = (int)(Math.random()*25)+36; (B) int rn = (int)(Math.random()*25)+60; (C)int rn = (int)(Math.random())*26)+60; (D)int rn = (int)(Math.random())*36+25;(E)int rn=(int)(Math.random()*60+25);\n",
    "1.C: Consider the following class definition. public class Example{ private int x;//constructor not shown} Which of the following is a correct header for a method of the Example class that would return the value of the private instance variable x so that it can be used in a class other than Example?\n",
    "2.A: Consider  the following  code  segment.  Assume  num is a properly  declared  and initialized  int variable.  if (num  > 0) { if (num  % 2 == 0) { System.out.println( \"A\"); } else  { System.out.println( \"B\"); } } Which  of the following  best describes  the result  of executing  the code  segment?  (A) When  num is a negative  odd integer,  \"B\" is printed;  otherwise,  \"A\" is printed.  (B) When  num is a negative  even  integer,  \"B\" is printed;  otherwise,  nothing  is printed.  (C) When  num is a positive  even  integer,  \"A\" is printed;  otherwise,  \"B\" is printed.  (D) When  num is a positive  even  integer,  \"A\" is printed;  when  num is a positive  odd integer,  \"B\" is printed;  otherwise,  nothing  is printed.\n",
    "2.A: Consider the following code segment.  int a=5; int b=2; double c=3.0; System.out.println(5+a/b*c-1); What is printed when the code segment is executed? (A) 0.6666666667 (B) 9.0  (C) 10.0  (D) 11.5  (E) 14.0\n",
    "2.A: Consider the following code segment. int x = 7;  int y = 3;  if ((x < 10) && (y < 0))  System.out.println(\"Value is: \" + x * y);  else  System.out.println(\"Value is: \" + x / y);  What is printed as a result of executing the code segment? (A) Value is: 21 (B) Value is: 2.3333333 (C) Value is: 2 (D) Value is: 0 \n",
    "2.B: Consider  the following  code segment.  int[][]  values  = {{1,  2, 3}, {4, 5, 6}};  int x = 0; for (int  j = 0; j < values.length;  j++)  { for (int  k = 0; k < values[0].length;  k++)  { if (k == 0) { values[j][k]  *= 2; } x += values[j][k];  } } What  is the value  of x after the code segment  is executed?  (A) 7 (B) 17 (C) 21 (D) 26\n",
    "2.B: Consider the following code segment. int[] arr = {7  2  5  3  0  10};  for (int k = 0; k < arr.length - 1; k++)  {  if (arr[k] > arr[k + 1])  System.out.print(k + \" \" + arr[k] + \" \");  }  What will be printed as a result of executing the code segment? (A) 0  2  2  3  3  0  (B) 0  7  2  5  3  3  (C) 0  7  2  5  5  10  (D) 1  7  3  5  4  3  \n",
    "2.B: Consider the following code segment. int[] arr = {1  2  3  4  5  6  7};  for (int k = 3; k < arr.length - 1; k++)  arr[k] = arr[k + 1];  Which of the following represents the contents of  arr as a result of executing the code segment? (A) {1  2  3  4  5  6  7} (B) {1  2  3  5  6  7} (C) {1  2  3  5  6  7  7} (D) {1  2  3  5  6  7  8} \n",
    "2.C: Consider  the following  code segment.  ArrayList<Integer>  numList  = new ArrayList<Integer>();  numList.add(3);  numList.add(2);  numList.add(1);  numList.add(1,  0); numList.set(0,  2); System.out.print(numList);  What  is printed  by the code segment?  (A) [1, 3, 0, 1]     (B) [2, 0, 2, 1](C) [2, 0, 2, 3](D) [2, 3, 2, 1]\n",
    "2.C: Consider the following method. public static int mystery(int[] arr)  {  int x = 0;  for (int k = 0; k < arr.length; k = k + 2)  x = x + arr[k];  return x;  }  Assume that the array  nums has been declared and initialized as follows. int[] nums = {3  6  1  0  1  4  2};  What value will be returned as a result of the call  mystery(nums) ? (A) 5  (B) 6  (C) 7  (D) 1 0  \n",
    "2.C: Consider the following instance variable and method. private List<String>  animals; public void manipulate()  {  for (int k = animals.size() - 1; k > 0; k--)  {  if (animals.get(k).substring(0  1).equals(\"b\"))  {  animals.add(animals.size() - k  animals.remove(k));  }  }  }    Assume  that  animals has been instantiated and initialized  with the following contents.  [\"bear\"  \"zebra\"  \"bass\"  \"cat\"  \"koala\"  \"baboon\"]  What will the contents of  animals be as a result of calling  manipulate ? (A) [\"baboon\"  \"zebra\"  \"bass\"  \"cat\"  \"bear\"  \"koala\"] (B) [\"bear\"  \"zebra\"  \"bass\"  \"cat\"  \"koala\"  \"baboon\"] (C) [\"baboon\"  \"bear\"  \"zebra\"  \"bass\"  \"cat\"  \"koala\"] (D) [\"bear\"  \"baboon\"  \"zebra\"  \"bass\"  \"cat\"  \"koala\"] \n",
    "2.D: Consider  the following  code segment.  String[]  testTwo  = {\"last \" , \"day\" , \"of\" , \"the\" , \"school \" , \" year \"}; String[]  resultTwo  = strArrMethod(testTwo);  How  many  times  is the line labeled  // Line 12 in the strArrMethod  executed  as a result  of executing  the code segment?\n",
    "2.D: Consider  the following  code  segment.  Assume  num is a properly  declared  and initialized  int variable.  if (num  > 0) { if (num  % 2 == 0) { System.out.println( \"A\"); } else  { System.out.println( \"B\"); } } Which  of the following  best describes  the result  of executing  the code  segment?  (A) When  num is a negative  odd integer   \"B\" is printed;  otherwise   \"A\" is printed.  (B) When  num is a negative  even  integer   \"B\" is printed;  otherwise   nothing  is printed.  (C) When  num is a positive  even  integer   \"A\" is printed;  otherwise   \"B\" is printed.  (D) When  num is a positive  even  integer   \"A\" is printed;  when  num is a positive  odd integer   \"B\" is printed;  otherwise   nothing  is printed.  \n",
    "4.A: Consider  the following  method.  public  static  void  printSome(int  num1,  int num2)  { for (int  i = 0; i < num1;  i++)  { if ( i % num2 == 0 && i % 2 == 0)  { System.out.print(i  + \"\"); } } } Which  of the following  method  calls will cause  \"0 10 \" to be printed?  (A) printSome(0,  20) (B) printSome(5,  10) (C) printSome(10,  5) (D) printSome(20,  5)\n",
    "4.A: Vehicles are classified based on their total interior volume. The classify method is intended to return a vehicle classification String value based on total interior volume, in cubic feet, as shown in the table below.  The classify method, which does not work as intended, is shown below. public static String classify(int volume) {  String carClass = \"\";if (volume >= 120){carClass = \"Large\";}else if (volume < 120){carClass = \"Mid-Size\";}else if (volume < 110){carClass = \"Compact\";}else if (volume < 100){ carClass = \"Subcompact\";}  The classify method works as intended for some but not all values of hte parameter volume.  For which of the following values of volume would the correct value be returned when the classify method is executed? (A) 80 (B) 90 (C) 105 (D) 109 (E) 115\n",
    "4.A: Consider  the following  recursive  method.  public  static  boolean  recurMethod(String  str)  { if (str.length()  <= 1) { return  true;  } else  if (str.substring(0   1).compareTo(str.substring(1   2)) > 0) { return  recurMethod(str.substring(1));  } else  { return  false;  } } Which  of the following  method  calls will return  true  ? (A) recurMethod( \"abcba \") (B) recurMethod( \"abcde \") (C) recurMethod( \"bcdab \") (D) recurMethod( \"edcba \") \n",
    "4.C: Consider  the following  code segment.  int num = /* initial  value  not shown  */; boolean  b1 = true;  if (num  > 0) { if (num  >= 100)  { b1 = false;  } } else  { if (num  >= -100)  { b1 = false;  } } Which  of the following  statements  assigns  the same  value  to b2 as the code segment  assigns  to b1 for all values  of num ? (A) boolean  b2 = (num  > -100)  && (num  < 100);  (B) boolean  b2 = (num  > -100)  || (num  < 100);  (C) boolean  b2 = (num  < -100)  || (num  > 100);  (D) boolean  b2 = (num  < -100)  && (num  > 0 || num < 100); \n",
    "4.C: Consider the following class declaration. public class Student  {  private String myName;  private int myAge;  public Student()  { /* implementation not shown */ }  public Student(String name  int age)  { /* implementation not shown */ }  // No ot her constructors  }  Which of the following declarations will compile without error? I. Student a = new Student(); II. Student b = new Student(\"Juan\"  15); III. Student c = new Student(\"Juan\"  \"15\"); (A) I  o n l y  (B) I I  o n l y  (C) I and II only (D) I and III only \n",
    "4.C: For which of the following test cases will the call  seqSearchRec(5) always  result in an error? I.  data contains only one element. II. data does not contain the value 5. III. data contains the value 5 multiple times. (A) I  o n l y  (B) I I  o n l y  (C) I I I  o n l y  (D) I and II only \n",
    "5.A: Consider  the following  code  segment.  Assume  num is a properly  declared  and initialized  int variable.  if (num  > 0) { if (num  % 2 == 0) { System.out.println( \"A\"); } else  { System.out.println( \"B\"); } } Which  of the following  best describes  the result  of executing  the code  segment?  (A) When  num is a negative  odd integer,  \"B\" is printed;  otherwise,  \"A\" is printed.  (B) When  num is a negative  even  integer,  \"B\" is printed;  otherwise,  nothing  is printed.  (C) When  num is a positive  even  integer,  \"A\" is printed;  otherwise,  \"B\" is printed.  (D) When  num is a positive  even  integer,  \"A\" is printed;  when  num is a positive  odd integer,  \"B\" is printed;  otherwise,  nothing  is printed.\n",
    "5.A: Consider  the following  code  segment.  Assume  that a is greater  than zero.  int a  = /*  value  not shown  */; int b = a +  (int)  (Math.random()  * a); Which  of the following  best describes  the value  assigned  to b when  the code  segment  is executed?  (A) a (B) 2*a  (C) A random  integer  between  0 and a-1   inclusive  (D) A random  integer  between  a and 2*a   inclusive  \n",
    "5.A: Consider  the following  code  segment.  Assume  that num3  > num2  > 0. int num1  = 0; int num2  = /* initial  value  not shown  */; int num3  = /* initial  value  not shown  */; while  (num2  < num3)  { num1  += num2;  num2++;  } Which  of the following  best describes  the contents  of num1  as a result  of executing  the code  segment?  (A) The product  of num2  and num3  (B) The product  of num2  and num3  -1 (C) The sum of num2  and num3  (D) The sum of all integers  from  num2  to num3   inclusive  \n",
    "5.B: The following  method  is intended  to remove  all elements  of an ArrayList  of integers  that are divisible  by key and add the removed  elements  to a new ArrayList , which  the method  returns.  public  static  ArrayList<Integer>  match(ArrayList<Integer>  numList,  int key)  { ArrayList<Integer>  returnList  = new ArrayList<Integer>();  int i = 0; while  (i < numList.size())  { int num = numList.get(i);  if (num  % key == 0) { numList.remove(i);  returnList.add(num);  } i++;  } return  returnList;  } As an example,  if the method  is called  with an ArrayList  containing  the values  [5, 2, 10, 20, 16] and the parameter  key has the value  5, then numList  should  contain  [2, 16] at the end of the method  and an ArrayList  containing  [5, 10, 20] should  be returned.  Which  of the following  best explains  why the method  does not always  work  as intended?  (A) The method  attempts  to add an element  to returnList  after that element  has already  been  removed  from  numList . (B) The method  causes  a NullPointerException  to be thrown  when  no matches  are found.  (C) The method  causes  an IndexOutOfBoundsException  to be thrown.  (D) The method  fails to correctly  determine  whether  an element  of numList  is divisible  by key.\n",
    "5.B: Consider the following  Book and AudioBook classes. public class Book  {  private int numPages;  private String bookTitle;  public Book(int pages  String title)  {  numPages = pages;  bookTitle = title;  }  public String toString()  {  return bookTitle + \" \" + numPages;  }  public int length()  {  return numPages;  }  }  public class AudioBook extends Book  {  private int numMinutes;  public AudioBook(int minutes  int pages  String title)  {  super(pages  title);  numMinutes = minutes;  }  public int length()  {  return numMinutes;  }  public double pagesPerMinute()  {  return ((double) super.length()) / numMinutes;  }  }  22               23GO ON TO THE NEXT PAGE.  Consider the following code segmen t that appears in a class other than  Book or AudioBook . Line 1: Book[] books = new Book[2];  Line 2: books[0] = new AudioBook(100  300  \"The Jungle\");  Line 3: books[1] = new Book(400  \"Captains Courageous\");  Line 4: System.out.println(books[0].pagesPerMinute());  Line 5: System.out.println(books[0].toString());  Line 6: System.out.println(books[0].length());  Line 7: System.out.println(books[1].toString());  Which of the following best explains  why the code segment will not compile? (A) Line 2 will not compile because variables of type  Book may not refer to variables of type  AudioBook . (B) Line 4 will not compile because variables of type  Book may only call methods in the  Book class. (C) Line 5 will not compile because the  AudioBook class does not have a method named  toString declared or implemented. (D) Line 6 will not compile because the statemen t is ambiguous. The compiler cannot determine which length method should be called.  \n",
    "5.B: Consider the following method, which is intended to return a list containing the elements of the parameter myList with all even elements removed. public static ArrayList<Integer> removeEvens (ArrayList<Integer> myList) { for (int i = 0; i < myList.size(); i++) { if (myList.get(i) % 2 == 0) { myList.remove(i); } } return myList; } Which of the following best explains why the code segment does not work as intended? (A) The code segment causes an IndexOutOfBoundsException for all lists because of an incorrect Boolean expression in the for loop. (B) The code segment causes an IndexOutOfBoundsException for lists with at least one even element because the indexes of all subsequent elements change by one when a list element is removed. (C) The code segment returns a list with fewer elements than intended because it fails to consider the last element of myList. (D) The code segment removes the wrong elements of myList because the condition in the if statement to test whether an element is even is incorrect. (E) The code segment skips some elements of myList because the indexes of all subsequent elements change by one when a list element is removed.\n",
    "5.D: Consider  the following  statement.  Assume  that a and b are properly  declared  and initialized  boolean  variables.  boolean  c = (a && b) || (!a && b); Under  which  of the following  conditions  will c be assigned  the value  false  ? (A) Always  (B) Never  (C) When  a and b have the same  value  (D) When  a has the value  false \n",
    "5.D: Consider  the following  methods.  /** Precondition: a>0  and b > 0 */  public  static  int methodOne(int  a  int b) { int loopCount  = 0; for (int i = 0; i < a / b; i++)  { loopCount++;  } return  loopCount;  } /** Precondition: a>0  and b > 0 */  public  static  int methodTwo(int  a  int b) { int loopCount  = 0; int i = 0; while  (i < a) { loopCount++;  i += b; } return  loopCount;  } Which  of the following  best describes  the conditions  under  which  methodOne  and methodTwo  return  the same  value?  (A) When  a and b are both even (B) When  a and b are both odd (C) When  a is even and b is odd (D) When  a  is equal  to zero \n",
    "Given the categories and examples above, which class does the following questions assess?  Give the result with just the class name.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=os.environ.get(\"jeff_api\"))\n",
    "\n",
    "def gpt_classify(prompt):\n",
    "  response = client.chat.completions.create(\n",
    "  model=\"gpt-4o-mini\",\n",
    "  messages=[{\"role\": \"user\", \"content\": prompt_start + prompt}]\n",
    "  )\n",
    "  return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ym/68nrz1n97wj0gz5413bhpqs80000gn/T/ipykernel_74229/3691975183.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"GPT_Classify\"] = df[\"Question\"].apply(gpt_classify)\n"
     ]
    }
   ],
   "source": [
    "df[\"GPT_Classify\"] = df[\"Question\"].apply(gpt_classify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.B       0.75      0.33      0.46        18\n",
      "         1.C       0.00      0.00      0.00         0\n",
      "         2.A       1.00      0.09      0.17        11\n",
      "         2.B       0.26      0.71      0.38        14\n",
      "         2.C       0.70      0.44      0.54        16\n",
      "         2.D       0.67      1.00      0.80        10\n",
      "         4.A       0.00      0.00      0.00        10\n",
      "         4.B       0.00      0.00      0.00         0\n",
      "         5.A       0.60      0.23      0.33        13\n",
      "         5.B       0.75      0.90      0.82        10\n",
      "        5.B:       0.00      0.00      0.00         0\n",
      "         5.C       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.45       102\n",
      "   macro avg       0.39      0.31      0.29       102\n",
      "weighted avg       0.60      0.45      0.44       102\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/jgoett/anaconda3/envs/jeff-env/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(df[\"Classification\"], df[\"GPT_Classify\"]));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "con_mat = confusion_matrix(df[\"Classification\"], df[\"GPT_Classify\"])\n",
    "cm = ConfusionMatrixDisplay(confusion_matrix=con_mat).plot()\n",
    "cm.figure_.savefig(\"Reports/Images/ChatGPT_ConfusionMatrix.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary  \n",
    "When just looking at two categories of questions, both the Logistic Regression and ChatGPT classifiers classify the computational thinking practice with 100% accuracy.  This should be expanded to incorporate classify questions from all 15 thinking practices.  In addition, it should be trained to classify the content assessed, not just the skill."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jeff-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
